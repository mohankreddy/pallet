#Our task list from which we coordinate with one another and generate release notes.

= Reduce Complexity =
Any task listed here should take 1 hour to no more than 2 weeks work.  If a task listed will take more than 2 weeks work then it should be broken into sub-tasks that take less time.

= Tasks =
(%) - means the task needs to be broken down into several sub-tasks
== Not started (In order of priority) ==

 
 * The documentation of the following pipes have not started(%) 
   * Add Classifier Token Predictions
   * Add Classifier Token Predictions . Token Classifiers
   * Augmentable Feature Vector Add Conjunctions
   * Classification 2 Confidence Predicting Feature Vector
   * Directory 2 File Iterator
   * Feature Count Pipe
   * Feature Value String 2 Feature Vector
   * Feature Vector Sequence 2 Feature Vectors
   * Filter Empty Feature Vectors
   * Instance List Trim Features By Count
   * Noop
   * Pipe
   * Print Input
   * Print Token Sequence Features
   * Save Data In Source
   * Selective SGML 2 Token Sequence
   * Simple Tagger Sentence 2 String Tokenization
   * Token 2 Feature Vector
   * Token Sequence 2 Token Instances
   * Token Sequence Parse Feature String 
 * The unit testing of all the pipes.(%)
 * Sample Application using Data Import functionality.  
 * Data Classification:
   * Decision Trees.
     * Code Review.
     * Documentation and Tutorials.
   * C4.5 Decision Trees.
   * Balanced Winnow.
   * AdaBoost?.
   * Bootstrap Aggregating (bagging)
   * Machine Learning Ensemble.
   * Rank Maximum Entropy
   * Winnow
   * Confidence Predicting.
   * Maximum Entropy models with Generalized Expectation Criteria.
  * Data classification unit tests
  * Real Application to show the Data Import and Classification functionality.

== Currently being worked on ==
  * Currently working on the following pipes.
   * Token Sequence Remove Non Alpha
   * Token Sequence NGrams
   * SGML2 Token Sequence
   * Target 2 Feature Sequence
   * Target 2 Label Sequence
   * Token Sequence 2 Feature Vector Sequence
   * Token Sequence Feature Parse Feature String
   * Token Sequence Match Data And Target
   * Token Sequence 2FeatureSequenceWithBigrams
   * Target Remeber Last Label
   * Source Location 2 Token Sequence 
  
  * Maximum Entropy 
    * Code Review - Currently have paused this work and have moved onto Decision Tree
    * Documentation and Tutorials

== Completed ==
  * The following task has been completed on the pipes(mentioned below)
    * Understanding the Algorithms
    * Documenting them
    * Small sample source code for each pipe.
    * Documenting everything in the form of small tutorials
    * Source code and Documentation Reviewed
     * File name 2 Char Sequence
     * Char Sequence Array 2 Token Sequence 
     * Csv 2 Array
     * Array 2 Feature Vector
     * Csv 2 Feature Vector
     * Target String To Features
     * Simple Tagger Sentence 2 Token Sequence
     * Simple Tokenizer
     * Pipe utils
     * String Add New Line Delimiter
     * Char Sequence Remove UUEncodedBlocks
     * String List 2 Feature Sequence
     * Feature Sequence Convolution
     * Feature Vector Conjunction
     * Input 2 Char Sequence
     * Char Sequence Replace 
     * Char Sequence Remove HTML 
     * Char Sequence 2 Token Sequence
     * Token Sequence Remove Stopwords
     * Make Ampersand XML Friendly
     * Token Sequence 2 Feature Sequence
     * Target 2 Label
     * Feature Sequence 2 Augmentable Feature Vector
     * Augmentable Feature Vector Log Scale
     * Char Sequence 2 charngrams
     * Char Sequence Lower case
     * Char Sub sequence
     * Line Group String 2 Token Sequence
     * Input 2 Char Sequence
     * Char Sequence 2 Token Sequence
     * Token Sequence Lower case
     * Token Sequence Remove Stopwords
     * Target 2 Label
     * Feature Sequence 2 Feature Vector
     * Print Input And Target 

  * Naive Bayes Algorithm
    * Understanding the algorithm
    * Reviewing Mallet code to understand the implementation.
    * Documentation of the mallet implementation of the algorithm and its tutorials.
  * Maximum Entropy
    * Understanding the algorithm.