#summary Document demonstrating the Document Classification algorithms of MALLET
#labels MalletDocumentClassification

Before proceeding any further, I want you to keep this in, This tutorial is not intended to describe the algorithms, it just aims at exposing the MALLET interfaces. 

= Introduction =
MALLET supports rich set of Document Classification Algorithms:
 * Naïve Bayes.
 * Maximum Entropy (Multivariate Logistic Regression).
 * C4.5 Decision Trees.
 * Decision Trees.
 * Balanced Winnow.
 * AdaBoost.
 * Bootstrap Aggregating (bagging)
 * Machine Learning Ensemble.
 * Rank Maximum Entropy
 * Winnow
 * Confidence Predicting.
 * Maximum Entropy models with Generalized Expectation Criteria.

Every Classifier is implemented as a Trainer and Classifier, and performs classification only on the instances processed with the pipe (import data pipe) associated with this classifier. Instances are generally Feature Vectors. Trainer is responsible for training data and the Classifier performs the classification based on the trained data.

Every Trainer consists train () method, which is used to train the data. A call to this train() method instantiates the Classifier which can classify the data based on its training.
Further, all Trainers and Classifiers can be found in cc.mallet.classify. Also, each trainer and classifier is implemented as an object in the MALLET. All the trainer mentioned above extends _ClassifierTrainer_ class and the trainers mentioned extends Classifier respectively, both of which can be found at *cc.mallet.classify*

Given, the general introduction on Classifier, let us proceed to explore each of the Algorithms.


=Naïve Bayes=
As with every classifier Naïve Bayes is also implemented as a trainer and classifier. Naïve Bayes assumes the Conditional Model to classify data.
{{{
	p(Classification|Data) = p(Data|Classification)p(Classification)/p(Data)

Further, Naïve Bayes also assumes the conditional independence between the data.

	p(Data|Classification) = p(d1,d2,..dn | Classification)
	p(d1,d2,...dn | Classification) = p(d1|Classification)p(d2|Classification)
}}}


Naïve Bayes Trainer and classifier are under the nomenclature NaiveBayesTrainer and NaiveBayesClassifier respectively and can be found at *cc.mallet.classify*.

{{{
NaïveBayes Trainer:
Extends    - ClassifierTrainer<NaiveBayes>
Implements - ClassifierTrainer.ByInstanceIncrements<NaiveBayes>, 
	     Boostable,AlphabetCarrying and java.io.Serializable.
}}}


Before, getting into details of Naïve Bayes Classifier, Let us look at a sample code for NaiveBayes classification (should also work for all the other classification algorithms).


{{{
class AppNaiveBayes
{
           // Look at Import Data Document for BuildPipe and Instance Lists description
        public static BuildPipe buildPipe ()
	{
	    String [][][] trainingdata = new String [][][] {
	    {{ "on the plains of africa the lions roar",
	    "in swahili ngoma means to dance",
	    "nelson mandela became president of south africa",
	    "the saraha dessert is expanding"}, {"africa"}},

            {{ "panda bears eat bamboo",
	    "china's one child policy has resulted in a surplus of boys",
	    "tigers live in the jungle"}, {"asia"}},

	    {{ "home of kangaroos",
	    "Autralian's for beer - Foster",
	    "Steve Irvin is a herpetologist"}, {"australia"}}};
		
	    BuildPipe bpipe = new BuildPipe ();		
	    Pattern tokenPattern = Pattern.compile("[\\p{L}\\p{N}_]+");
	    try {
		bpipe.CreatePipe( new Input2CharSequence("UTF-8"),
			new CharSequence2TokenSequence(tokenPattern), 
			true, 
			new TokenSequenceRemoveStopwords(true, true), 
			new TokenSequence2FeatureSequence(), 
			new Target2Label(), 
			new FeatureSequence2FeatureVector(), 
			false);
	    } catch (IOException e ) {};
		
	    for (int i = 0; i < 3; i++) 
            {
		try {
		  bpipe.addThruPipe (
                       new ArrayIterator (trainingdata[i][0],trainingdata[i][1][0]));
		} catch (Exception e) { }
	    }
	    return bpipe;
	}

	public static void main (String Args []) {				
		// 1. Create the instance list
		BuildPipe bpipe = AppNaiveBayes.buildPipe();
		// 2. Create NaiveBayes Trainer
		NaiveBayesTrainer trainer = new NaiveBayesTrainer ();
		// 3. Train the trainer with the data
		NaiveBayes cl = trainer.train(bpipe.GetInstanceList());
		// 4. Classify the sample data based on the trained data
  	        Classification c = cl.classify("nelson Mandela never eats lion");
 	        // 5. Validation of correctness
	        System.out.println (“Class Name – “ + 
					c.getLabeling().getBestLabel());
        }
}
}}}


Above source code shows the steps involved in the MALLET document classification.
{{{
1.Instance list of training data is created using the BuildPipe object.
2.Instantiation of NaiveBayesTrainer.
3.Training the classifier with the instance list created. 
As you can see the train method returns an instance of NaiveBayes Classifier.
(more detailed explanation in Instance method section of this document).
4.Classification of new data on the trained data is achieved using the method: NaiveBayes.classify (Object obj).
(more detailed explanation in Instance method section of this document).
}}}

*Note - look at the Import Data document for more info on Importing Data, Instance Lists, and Iterators.


===Instance Methods:===


====Constructors:====
{{{
1.NaiveBayesTrainer ()
2.NaiveBayesTrainer (Pipe instancePipe)
3.NaiveBayesTrainer (NaiveBayes initialClassifier)
Used to initialize trainer with an existing Classifier.
When initialized with an existing classifier, a trainer need not train on the same training data
}}}


====Public Instance Methods:====
{{{
1.NaiveBayes getClassifier ()
  Returns instance of the NaiveBayes classifier.

2.NaiveBayes train (InstanceList trainingList)
  Trains the naivebayes trainer with the training list passed as an Instance List of
  Feature Vector, all previous internal states are reset when train returns.

3.NaiveBayes trainIncremental (InstanceList trainingList)
  Same as the train method but does not resets the internal state.
  Incrementally adds the count of the training data and Training data is lnstance list of
  Feature Vector.

4.NaiveBayes trainIncremental(Instance instance)
  Same as previous incremental trainer with training data as an Instance.

5.NaiveBayesTrainer setDocLengthNormalization(double d)
  Sets Documentation Length Normalization.

6.Multinomial.Estimator getFeatureMultinomialEstimator()
  Returns the multinomial Estimator that is used for document classification.

7.Multinomial.Estimator getPriorMultinomialEstimator()

8.NaiveBayesTrainer setPriorMultinomialEstimator(Multinomial.Estimator me)

9.Alphabet getAlphabet()

10.Alphabet[] getAlphabets()

11.alphabetsMatch(AlphabetCarrying object) 
Alphabet maps integers and objects where the mapping in each direction is efficient.
(Will be explained in detail under the section Alphabet.

12.String toString()
}}}


===Code snippet for incremental training:===

{{{
   // Create Feature Vector Instance List as shown in the code above
   // Refer Import Data document for more info
   // Assume Instance list is created by previous code snippet

   NaiveBayesTrainer trainer = new NaiveBayesTrainer ();
   NaiveBayes classifier = trainer.train(pipe.GetInstanceList());
		
   Classification c = classifier.classify("nelson mandela never eats lions");
   System.out.println("Class Name - " + c.getLabeling().getBestLabel());
		
   c = classifier.classify("Steve irvin stays in australia");
   System.out.println("Class Name - " + c.getLabeling().getBestLabel());
		
   String [][][] incTrainingdata = new String [][][] {
   {{"Tiger woods is golf master",
     "The Niagara Falls are voluminous waterfalls on the Niagara River",
     "Christopher Columbus discovered American Continents"}, {"America"}}};
		
   pipe.addThruPipe (
    new ArrayIterator (incTrainingdata[0][0],incTrainingdata[0][1][0]));		
	 
   classifier = trainer.trainIncremental(pipe.GetInstanceList());
   c = classifier.classify("Woods has won American golf championships");
   System.out.println("Class Name - " + c.getLabeling().getBestLabel());	

}}}


===Code Snippet for Reusing the existing Trained Classifier===

{{{
   NaiveBayesTrainer reuse_trainer = new NaiveBayesTrainer (classifier);
   NaiveBayes reuse_classifier = trainer.getClassifier();
   c = reuse_classifier.classify("Woods has won American golf championships");
   System.out.println("Class Name - " + c.getLabeling().getBestLabel());
}}}

All Trainers works the same way!!!

Ok,time to move on to next Trainer - *Maximum Entropy Algorithm*


=Maximum Entropy=

Again, as every other Classifier Maximum Entropy is also implemented as Trainer and Classifier. Nomenclature used in MALLET for Maximum Entropy:
 * Trainer - MaxEntTrainer 
 * Classifier - MaxEnt

Both [http://mallet.cs.umass.edu/api/ MaxEntTrainer] and [http://mallet.cs.umass.edu/api/ MaxEnt] could be found in [http://mallet.cs.umass.edu/api/ cc.mallet.classify]






*Yet to be completed*